{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from models import transformer_position\n",
    "from utils import return_pos_embeddings, return_pos_df\n",
    "from dataset import SeqContextDataset, SingleFrameContextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1272cd5d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = mps\n"
     ]
    }
   ],
   "source": [
    "MAX_FRAME_ID = 164\n",
    "TRAIN_SPLIT = 0.8\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "SAVE_DIR = f\"\"\n",
    "DATA_DIR = f\"\"\n",
    "\n",
    "NORM=True\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device = {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings = return_pos_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_position(pos_df=new_embeddings, feature_embed_size=128, dropout=0.35, num_encoder_layers=4, num_att_heads=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=10, max_iters=2000)\n",
    "lr_scheduler = ExponentialLR(optimizer, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'cleaned_data/{DATA_DIR}/game_play_id.json') as f:\n",
    "    list_IDS = json.load(f)\n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(list_IDS)\n",
    "\n",
    "# break ids into train-val-test sets\n",
    "val_percent = int(len(list_IDS)* (TRAIN_SPLIT+((1-TRAIN_SPLIT)/2)) )\n",
    "train_IDS, val_IDS, test_IDS = np.split(list_IDS, [ int(len(list_IDS)*TRAIN_SPLIT), val_percent ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 256,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "# Generators\n",
    "seq_training_set = SeqContextDataset(train_IDS, data_dir=DATA_DIR)\n",
    "seq_training_generator = torch.utils.data.DataLoader(seq_training_set, **params)\n",
    "\n",
    "val_params = {'batch_size': 4,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "seq_validation_set = SeqContextDataset(val_IDS, data_dir=DATA_DIR)\n",
    "seq_validation_generator = torch.utils.data.DataLoader(seq_validation_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2022100208_3722', '2022100202_2263', '2022103006_1856',\n",
       "       '2022101605_2298', '2022092510_55'], dtype='<U15')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_IDS[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_list = []\n",
    "if NORM==True:\n",
    "    training_mean, training_var, normalization_mask, min_max_diff = seq_training_set.get_normalize()\n",
    "    norm_list = [training_mean, training_var, normalization_mask, min_max_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_mean shape = torch.Size([253])\n",
      "normalization_mask shape = torch.Size([253])\n",
      "min_max_diff shape = torch.Size([1, 253])\n"
     ]
    }
   ],
   "source": [
    "print(f\"training_mean shape = {training_mean.shape}\")\n",
    "print(f\"normalization_mask shape = {normalization_mask.shape}\")\n",
    "print(f\"min_max_diff shape = {min_max_diff.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"saved_models/{SAVE_DIR}/weights/norm_list.pickle\", 'wb') as fp:\n",
    "   pickle.dump(norm_list, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_features, all_train_labels, all_train_context_vectors, all_train_ids = seq_training_set.get_all_features_and_labels()\n",
    "all_validation_features, all_validation_labels, all_val_context_vectors, all_val_ids = seq_validation_set.get_all_features_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new generator with single frames\n",
    "\n",
    "full_training_set = SingleFrameContextDataset(all_train_features, all_train_labels, all_train_context_vectors, all_train_ids)\n",
    "full_train_gen = torch.utils.data.DataLoader(full_training_set, **params)\n",
    "\n",
    "full_validation_set = SingleFrameContextDataset(all_validation_features, all_validation_labels, all_val_context_vectors, all_val_ids)\n",
    "full_validation_gen = torch.utils.data.DataLoader(full_validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len training set = 292033\n",
      "Len validation set = 37061\n",
      "Number of train batches = 1141\n",
      "Number of val batches = 145\n"
     ]
    }
   ],
   "source": [
    "print(f\"Len training set = {len(full_training_set)}\")\n",
    "print(f\"Len validation set = {len(full_validation_set)}\")\n",
    "\n",
    "print(f\"Number of train batches = {len(full_train_gen)}\")\n",
    "print(f\"Number of val batches = {len(full_validation_gen)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del seq_training_set\n",
    "#del training_generator\n",
    "del seq_validation_set\n",
    "#del validation_generator\n",
    "\n",
    "del all_train_features\n",
    "del all_train_labels\n",
    "del all_validation_features\n",
    "del all_validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Processes batch from generator for a single frame\n",
    "norm_list: if supplied, contains [training_mean, training_var, normalization_mask]\n",
    "'''\n",
    "def process_single_frame_no_seq_batch(model, device, batch, labels, context, player_ids, norm_list:list=[]):\n",
    "\n",
    "    local_batch = batch.to(device)\n",
    "    index_labels = torch.argmax(labels, -1).to(device) # (batch_size)\n",
    "    context = context.to(torch.float32).to(device)\n",
    "\n",
    "    # normalize, either mean/std or min_max\n",
    "    if len(norm_list) != 0:\n",
    "        training_mean, _, normalization_mask, min_max_diff = norm_list\n",
    "        training_mean = training_mean.to(device)\n",
    "        normalization_mask = normalization_mask.to(device)\n",
    "        min_max_diff = min_max_diff.to(device)\n",
    "        local_batch[:,normalization_mask] = (local_batch[:,normalization_mask] - training_mean[normalization_mask].reshape(1,-1))/torch.where(min_max_diff[:,normalization_mask]==0,1,min_max_diff[:,normalization_mask])\n",
    "\n",
    "    local_batch = local_batch.reshape(-1,23,11)\n",
    "\n",
    "    pos_embeddings = return_pos_embeddings(model.pos_df, player_ids).to(device)    # [batch_size, 23, embed_dim]\n",
    "\n",
    "    output = model(local_batch, pos_embeddings, context).to(device)  # (batch_size, 23)\n",
    "\n",
    "    batch_loss = F.nll_loss(output, index_labels, reduction='mean')\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_frame_no_seq_val_batch(model, device, batch, labels, context, lengths, player_ids, norm_list:list=[]):\n",
    "\n",
    "    local_batch = batch.to(device)\n",
    "    index_labels = torch.argmax(labels, -1).to(device) # (batch_size)\n",
    "    local_lengths = lengths.to(device)    # [batch_size, 23, embed_dim]\n",
    "    local_context = context.to(torch.float32).to(device)\n",
    "\n",
    "    batch_seq_loss = 0  # avg avg seq loss (e.g, on expected sequence, average loss)\n",
    "    val_metrics_dict = {\"quarter_output_pred\":0,\n",
    "                   \"halfway_output_pred\":0,\n",
    "                   \"three_quarter_output_pred\":0,\n",
    "                   \"final_output_pred\":0,\n",
    "                   \"correct_tackler_identified_w_highest_prob_anytime\":0,\n",
    "                   \"correct_tackler_had_highest_average_prob\":0,\n",
    "                   \"correct_tackler_average_prob\":0}\n",
    "\n",
    "    # for each full sequence\n",
    "    for i in range(0, local_batch.shape[0]):\n",
    "\n",
    "        single_target = index_labels[i, local_lengths[i]-1].to(device)                                 # ([]), the correct class\n",
    "        batch = local_batch[i, :local_lengths[i]-1, :].reshape(-1,23,11).to(device)    # (seq_length, 23, 11)\n",
    "        context = local_context[i,:local_lengths[i]-1,:].to(device)\n",
    "        pos_embeddings = return_pos_embeddings(model.pos_df, player_ids[i:i+1]).repeat(local_lengths[i]-1,1,1).to(device)\n",
    "\n",
    "        if len(norm_list) != 0:\n",
    "            training_mean, _, normalization_mask, min_max_diff = norm_list\n",
    "            training_mean = training_mean.to(device)\n",
    "            normalization_mask = normalization_mask.to(device)\n",
    "            min_max_diff = min_max_diff.to(device)\n",
    "\n",
    "            batch = batch.reshape(-1,23*11)\n",
    "            batch[:,normalization_mask] = (batch[:,normalization_mask] - training_mean[normalization_mask].reshape(1,-1))/torch.where(min_max_diff[:,normalization_mask]==0,1,min_max_diff[:,normalization_mask])\n",
    "            batch = batch.reshape(-1,23,11)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(batch, pos_embeddings, context).to(device)\n",
    "\n",
    "        batch_seq_loss += F.nll_loss(output, index_labels[i, :local_lengths[i]-1], reduction='mean')\n",
    "\n",
    "        val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] += ((output.argmax(dim=1) == single_target).sum() > 0).float().item()    #output.argmax(dim=1) = (seq_length)\n",
    "\n",
    "        avg_tackle_probs_over_seq = torch.exp(output.mean(dim=0))                                      # (24)\n",
    "        val_metrics_dict['correct_tackler_average_prob'] += avg_tackle_probs_over_seq[single_target].item()\n",
    "        val_metrics_dict['correct_tackler_had_highest_average_prob'] += (avg_tackle_probs_over_seq.argmax()==single_target).float().item()\n",
    "\n",
    "        # shape ([]), 1 or 0 if classified correctly at that point in time\n",
    "        val_metrics_dict['quarter_output_pred'] += (output[output.shape[0]//4, :].argmax() == single_target).float().item()\n",
    "        val_metrics_dict['halfway_output_pred'] += (output[output.shape[0]//2, :].argmax() == single_target).float().item()\n",
    "        val_metrics_dict['three_quarter_output_pred'] += (output[(output.shape[0]//4)*3, :].argmax() == single_target).float().item()\n",
    "        val_metrics_dict['final_output_pred'] += (output[-1, :].argmax() == single_target).float().item()\n",
    "\n",
    "    return batch_seq_loss, val_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Runs epoch w/ diff val \n",
    "'''\n",
    "def run_epoch(model, device, train_generator, val_generator, optimizer, num_val_batches=5, lr_scheduler=None, norm_list=[]):\n",
    "    model.to(device)\n",
    "    # model.train()\n",
    "    avg_total_loss = 0\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    val_metrics_dict = {\"quarter_output_pred\":0,\n",
    "                   \"halfway_output_pred\":0,\n",
    "                   \"three_quarter_output_pred\":0,\n",
    "                   \"final_output_pred\":0,\n",
    "                   \"correct_tackler_identified_w_highest_prob_anytime\":0,\n",
    "                   \"correct_tackler_had_highest_average_prob\":0,\n",
    "                   \"correct_tackler_average_prob\":0}\n",
    "    # train\n",
    "    for batch_index, (local_batch, local_labels, local_context, local_lengths, local_player_ids, local_ids) in enumerate(train_generator):\n",
    "        optimizer.zero_grad()\n",
    "        batch_train_loss = process_single_frame_no_seq_batch(model, device, local_batch, local_labels, local_context, local_player_ids, norm_list)\n",
    "        \n",
    "        train_loss_hist.append(batch_train_loss.item())\n",
    "        avg_total_loss += batch_train_loss.item()\n",
    "\n",
    "        batch_train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 250 == 0:\n",
    "            print(f\"batch {batch_index} train loss = {batch_train_loss}\")\n",
    "\n",
    "    avg_total_loss /= len(train_generator)\n",
    "    if lr_scheduler != None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    # check val\n",
    "    model.eval()\n",
    "    avg_val_loss = 0  # avg avg seq loss (e.g, on expected sequence, average loss)\n",
    "\n",
    "    gen = iter(val_generator)\n",
    "    for batch_index in range(num_val_batches):\n",
    "        with torch.no_grad():\n",
    "            local_batch, local_labels, local_context, local_lengths, local_player_ids, local_ids = next(gen)\n",
    "            # val_metrics[0] = val loss\n",
    "            batch_seq_loss, val_batch_metrics_dict = process_single_frame_no_seq_val_batch(model, device, local_batch, local_labels, local_context, local_lengths, local_player_ids, norm_list)\n",
    "            val_loss_hist.append(batch_seq_loss.item())\n",
    "\n",
    "            avg_val_loss += batch_seq_loss.item()\n",
    "            val_metrics_dict['quarter_output_pred'] += val_batch_metrics_dict['quarter_output_pred']\n",
    "            val_metrics_dict['halfway_output_pred'] += val_batch_metrics_dict['halfway_output_pred']\n",
    "            val_metrics_dict['three_quarter_output_pred'] += val_batch_metrics_dict['three_quarter_output_pred']\n",
    "            val_metrics_dict['final_output_pred'] += val_batch_metrics_dict['final_output_pred']\n",
    "            val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] += val_batch_metrics_dict['correct_tackler_identified_w_highest_prob_anytime']\n",
    "            val_metrics_dict['correct_tackler_had_highest_average_prob'] += val_batch_metrics_dict['correct_tackler_had_highest_average_prob']\n",
    "            val_metrics_dict['correct_tackler_average_prob'] += val_batch_metrics_dict['correct_tackler_average_prob']\n",
    "\n",
    "    avg_val_loss /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['quarter_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['halfway_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['three_quarter_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['final_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_had_highest_average_prob'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_average_prob'] /= (num_val_batches*val_generator.batch_size)\n",
    "\n",
    "    return avg_total_loss, avg_val_loss, train_loss_hist, val_loss_hist, val_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting training...\")\n",
    "train_batch_loss_hist=[]\n",
    "val_batch_loss_hist=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/4wq7jf9n3c74v00gxl_2v81r0000gn/T/ipykernel_45054/2075596127.py:17: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:283.)\n",
      "  local_batch[:,normalization_mask] = (local_batch[:,normalization_mask] - training_mean[normalization_mask].reshape(1,-1))/torch.where(min_max_diff[:,normalization_mask]==0,1,min_max_diff[:,normalization_mask])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 train loss = 3.1311018466949463\n",
      "batch 250 train loss = 1.9542165994644165\n",
      "batch 500 train loss = 1.907496690750122\n",
      "batch 750 train loss = 1.6545021533966064\n",
      "batch 1000 train loss = 1.7891631126403809\n",
      "Epoch=0: train_loss=1.9020997888680407, val_loss=1.428937601191657. LR=[0.00015]\n",
      "val metrics dict = \n",
      "['quarter_output_pred', 'halfway_output_pred', 'three_quarter_output_pred', 'final_output_pred', 'correct_tackler_identified_w_highest_prob_anytime', 'correct_tackler_had_highest_average_prob', 'correct_tackler_average_prob']\n",
      "[0.375 0.393 0.589 0.607 0.911 0.536 0.332]\n",
      "#######################\n",
      "batch 0 train loss = 1.6830190420150757\n",
      "batch 250 train loss = 1.4330084323883057\n",
      "batch 500 train loss = 1.4928847551345825\n",
      "batch 750 train loss = 1.4359923601150513\n",
      "batch 1000 train loss = 1.462562918663025\n",
      "Epoch=1: train_loss=1.4452898280856694, val_loss=1.3313546904495783. LR=[7.5e-05]\n",
      "val metrics dict = \n",
      "['quarter_output_pred', 'halfway_output_pred', 'three_quarter_output_pred', 'final_output_pred', 'correct_tackler_identified_w_highest_prob_anytime', 'correct_tackler_had_highest_average_prob', 'correct_tackler_average_prob']\n",
      "[0.429 0.589 0.732 0.571 0.893 0.625 0.401]\n",
      "#######################\n",
      "batch 0 train loss = 1.250624656677246\n",
      "batch 250 train loss = 1.3680715560913086\n",
      "batch 500 train loss = 1.1153982877731323\n",
      "batch 750 train loss = 1.2673431634902954\n",
      "batch 1000 train loss = 1.1021296977996826\n",
      "Epoch=2: train_loss=1.1885767031092063, val_loss=1.3127692682402474. LR=[3.75e-05]\n",
      "val metrics dict = \n",
      "['quarter_output_pred', 'halfway_output_pred', 'three_quarter_output_pred', 'final_output_pred', 'correct_tackler_identified_w_highest_prob_anytime', 'correct_tackler_had_highest_average_prob', 'correct_tackler_average_prob']\n",
      "[0.464 0.571 0.607 0.75  0.982 0.696 0.397]\n",
      "#######################\n",
      "batch 0 train loss = 0.9866114258766174\n",
      "batch 250 train loss = 1.016887903213501\n",
      "batch 500 train loss = 0.9671285152435303\n",
      "batch 750 train loss = 0.8686762452125549\n",
      "batch 1000 train loss = 0.9467177391052246\n",
      "Epoch=3: train_loss=0.9451064377579117, val_loss=1.9148177717413222. LR=[1.875e-05]\n",
      "val metrics dict = \n",
      "['quarter_output_pred', 'halfway_output_pred', 'three_quarter_output_pred', 'final_output_pred', 'correct_tackler_identified_w_highest_prob_anytime', 'correct_tackler_had_highest_average_prob', 'correct_tackler_average_prob']\n",
      "[0.304 0.429 0.661 0.75  0.946 0.625 0.332]\n",
      "#######################\n",
      "batch 0 train loss = 0.9211246967315674\n",
      "batch 250 train loss = 0.8435168266296387\n",
      "batch 500 train loss = 0.8292601108551025\n",
      "batch 750 train loss = 0.7602986097335815\n",
      "batch 1000 train loss = 0.7166743874549866\n",
      "Epoch=4: train_loss=0.779465665428603, val_loss=2.3906140455177853. LR=[9.375e-06]\n",
      "val metrics dict = \n",
      "['quarter_output_pred', 'halfway_output_pred', 'three_quarter_output_pred', 'final_output_pred', 'correct_tackler_identified_w_highest_prob_anytime', 'correct_tackler_had_highest_average_prob', 'correct_tackler_average_prob']\n",
      "[0.339 0.411 0.571 0.643 0.929 0.643 0.318]\n",
      "#######################\n",
      "Finished training 5 epochs in 26.611 min\n"
     ]
    }
   ],
   "source": [
    "total_start_time = time.time()\n",
    "\n",
    "for epoch_index in range(NUM_EPOCHS):\n",
    "    train_loss, val_loss, tr_hist, val_hist, val_metrics_dict = run_epoch(model, DEVICE, full_train_gen, seq_validation_generator, \\\n",
    "                                    optimizer, lr_scheduler=lr_scheduler, num_val_batches=14, norm_list=norm_list)\n",
    "\n",
    "    train_batch_loss_hist += tr_hist\n",
    "    val_batch_loss_hist += val_hist\n",
    "    print(f\"Epoch={epoch_index}: train_loss={train_loss}, val_loss={val_loss}. LR={lr_scheduler.get_last_lr()}\")\n",
    "    print(f\"val metrics dict = \")\n",
    "    print(f\"{list(val_metrics_dict.keys())}\")\n",
    "    print(f\"{np.array(list(val_metrics_dict.values())).round(3)}\")\n",
    "    print(f\"#######################\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch_index,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss\n",
    "    }, f\"./saved_models/{SAVE_DIR}/weights/checks/model_checkpoint_{epoch_index}.pt\")\n",
    "    \n",
    "    \n",
    "total_end_time = time.time()\n",
    "print(f\"Finished training {NUM_EPOCHS} epochs in {round((total_end_time - total_start_time)/60, 3)} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdb_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
