{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from models import trans_single_frame_13_6_nine, trans_single_frame_13_6_hw\n",
    "from utils import return_pos_embeddings, return_pos_df\n",
    "from dataset import SeqContextDataset, SingleFrameContextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FRAME_ID = 164\n",
    "TRAIN_SPLIT = 0.8\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "DATA_DIR = f\"seq_clipped_sorted_data\"\n",
    "\n",
    "NORM=True\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device = {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = f\"trans_single_frame_13_6_hw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings = return_pos_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = new_embeddings.loc[:,['0', '1']].max(axis=0).values\n",
    "min = new_embeddings.loc[:,['0', '1']].min(axis=0).values\n",
    "mean = new_embeddings.loc[:,['0', '1']].mean(axis=0).values\n",
    "new_embeddings.loc[:,['0','1']] = (new_embeddings.loc[:,['0','1']] - mean)/(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trans_single_frame_13_6_hw(pos_df=new_embeddings, feature_embed_size=128, dropout=0.35, num_encoder_layers=4, num_att_heads=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"./saved_models/trans_single_frame_12_12_pos/weights/trans_all_players.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=10, max_iters=2000)\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "lr_scheduler = ExponentialLR(optimizer, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'cleaned_data/{DATA_DIR}/game_play_id.json') as f:\n",
    "    list_IDS = json.load(f)\n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(list_IDS)\n",
    "\n",
    "# break ids into train-val-test sets\n",
    "val_percent = int(len(list_IDS)* (TRAIN_SPLIT+((1-TRAIN_SPLIT)/2)) )\n",
    "train_IDS, val_IDS, test_IDS = np.split(list_IDS, [ int(len(list_IDS)*TRAIN_SPLIT), val_percent ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 256,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "# Generators\n",
    "seq_training_set = SeqContextDataset(train_IDS, data_dir=DATA_DIR)\n",
    "seq_training_generator = torch.utils.data.DataLoader(seq_training_set, **params)\n",
    "\n",
    "val_params = {'batch_size': 4,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "seq_validation_set = SeqContextDataset(val_IDS, data_dir=DATA_DIR)\n",
    "seq_validation_generator = torch.utils.data.DataLoader(seq_validation_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_IDS[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_list = []\n",
    "if NORM==True:\n",
    "    training_mean, training_var, normalization_mask, min_max_diff = seq_training_set.get_normalize()\n",
    "    norm_list = [training_mean, training_var, normalization_mask, min_max_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training_mean shape = {training_mean.shape}\")\n",
    "print(f\"normalization_mask shape = {normalization_mask.shape}\")\n",
    "print(f\"min_max_diff shape = {min_max_diff.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"saved_models/{SAVE_DIR}/weights/norm_list.pickle\", 'wb') as fp:\n",
    "   pickle.dump(norm_list, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_features, all_train_labels, all_train_context_vectors, all_train_ids = seq_training_set.get_all_features_and_labels()\n",
    "all_validation_features, all_validation_labels, all_val_context_vectors, all_val_ids = seq_validation_set.get_all_features_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new generator with single frames\n",
    "\n",
    "full_training_set = SingleFrameContextDataset(all_train_features, all_train_labels, all_train_context_vectors, all_train_ids)\n",
    "full_train_gen = torch.utils.data.DataLoader(full_training_set, **params)\n",
    "\n",
    "full_validation_set = SingleFrameContextDataset(all_validation_features, all_validation_labels, all_val_context_vectors, all_val_ids)\n",
    "full_validation_gen = torch.utils.data.DataLoader(full_validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_batch, local_labels, local_context, local_lengths, local_player_ids, local_ids = full_training_set.__getitem__(full_training_set.__len__()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_player_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Len training set = {len(full_training_set)}\")\n",
    "print(f\"Len validation set = {len(full_validation_set)}\")\n",
    "\n",
    "print(f\"Number of train batches = {len(full_train_gen)}\")\n",
    "print(f\"Number of val batches = {len(full_validation_gen)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del seq_training_set\n",
    "#del training_generator\n",
    "del seq_validation_set\n",
    "#del validation_generator\n",
    "\n",
    "del all_train_features\n",
    "del all_train_labels\n",
    "del all_validation_features\n",
    "del all_validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Processes batch from generator for a single frame\n",
    "norm_list: if supplied, contains [training_mean, training_var, normalization_mask]\n",
    "'''\n",
    "def process_single_frame_no_seq_batch(model, device, batch, labels, context, player_ids, norm_list:list=[]):\n",
    "\n",
    "    local_batch = batch.to(device)\n",
    "    index_labels = torch.argmax(labels, -1).to(device) # (batch_size)\n",
    "    context = context.to(torch.float32).to(device)\n",
    "\n",
    "    # normalize, either mean/std or min_max\n",
    "    if len(norm_list) != 0:\n",
    "        training_mean, _, normalization_mask, min_max_diff = norm_list\n",
    "        training_mean = training_mean.to(device)\n",
    "        normalization_mask = normalization_mask.to(device)\n",
    "        min_max_diff = min_max_diff.to(device)\n",
    "        local_batch[:,normalization_mask] = (local_batch[:,normalization_mask] - training_mean[normalization_mask].reshape(1,-1))/torch.where(min_max_diff[:,normalization_mask]==0,1,min_max_diff[:,normalization_mask])\n",
    "\n",
    "    local_batch = local_batch.reshape(-1,23,11)\n",
    "\n",
    "    pos_embeddings = return_pos_embeddings(model.pos_df, player_ids).to(device)    # [batch_size, 23, embed_dim]\n",
    "\n",
    "    output = model(local_batch, pos_embeddings, context).to(device)  # (batch_size, 23)\n",
    "\n",
    "    batch_loss = F.nll_loss(output, index_labels, reduction='mean')\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_frame_no_seq_val_batch(model, device, batch, labels, context, lengths, player_ids, norm_list:list=[]):\n",
    "\n",
    "    local_batch = batch.to(device)\n",
    "    index_labels = torch.argmax(labels, -1).to(device) # (batch_size)\n",
    "    local_lengths = lengths.to(device)    # [batch_size, 23, embed_dim]\n",
    "    local_context = context.to(torch.float32).to(device)\n",
    "\n",
    "    batch_seq_loss = 0  # avg avg seq loss (e.g, on expected sequence, average loss)\n",
    "    val_metrics_dict = {\"quarter_output_pred\":0,\n",
    "                   \"halfway_output_pred\":0,\n",
    "                   \"three_quarter_output_pred\":0,\n",
    "                   \"final_output_pred\":0,\n",
    "                   \"correct_tackler_identified_w_highest_prob_anytime\":0,\n",
    "                   \"correct_tackler_had_highest_average_prob\":0,\n",
    "                   \"correct_tackler_average_prob\":0}\n",
    "\n",
    "    # for each full sequence\n",
    "    for i in range(0, local_batch.shape[0]):\n",
    "\n",
    "        single_target = index_labels[i, local_lengths[i]-1].to(device)                                 # ([]), the correct class\n",
    "        batch = local_batch[i, :local_lengths[i]-1, :].reshape(-1,23,11).to(device)    # (seq_length, 23, 11)\n",
    "        context = local_context[i,:local_lengths[i]-1,:].to(device)\n",
    "        pos_embeddings = return_pos_embeddings(model.pos_df, player_ids[i:i+1]).repeat(local_lengths[i]-1,1,1).to(device)\n",
    "\n",
    "        if len(norm_list) != 0:\n",
    "            training_mean, _, normalization_mask, min_max_diff = norm_list\n",
    "            training_mean = training_mean.to(device)\n",
    "            normalization_mask = normalization_mask.to(device)\n",
    "            min_max_diff = min_max_diff.to(device)\n",
    "\n",
    "            batch = batch.reshape(-1,23*11)\n",
    "            batch[:,normalization_mask] = (batch[:,normalization_mask] - training_mean[normalization_mask].reshape(1,-1))/torch.where(min_max_diff[:,normalization_mask]==0,1,min_max_diff[:,normalization_mask])\n",
    "            batch = batch.reshape(-1,23,11)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(batch, pos_embeddings, context).to(device)\n",
    "\n",
    "        batch_seq_loss += F.nll_loss(output, index_labels[i, :local_lengths[i]-1], reduction='mean')\n",
    "\n",
    "        val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] += ((output.argmax(dim=1) == single_target).sum() > 0).float().item()    #output.argmax(dim=1) = (seq_length)\n",
    "\n",
    "        avg_tackle_probs_over_seq = torch.exp(output.mean(dim=0))                                      # (24)\n",
    "        val_metrics_dict['correct_tackler_average_prob'] += avg_tackle_probs_over_seq[single_target].item()\n",
    "        val_metrics_dict['correct_tackler_had_highest_average_prob'] += (avg_tackle_probs_over_seq.argmax()==single_target).float().item()\n",
    "\n",
    "        # shape ([]), 1 or 0 if classified correctly at that point in time\n",
    "        val_metrics_dict['quarter_output_pred'] += (output[output.shape[0]//4, :].argmax() == single_target).float().item()\n",
    "        val_metrics_dict['halfway_output_pred'] += (output[output.shape[0]//2, :].argmax() == single_target).float().item()\n",
    "        val_metrics_dict['three_quarter_output_pred'] += (output[(output.shape[0]//4)*3, :].argmax() == single_target).float().item()\n",
    "        val_metrics_dict['final_output_pred'] += (output[-1, :].argmax() == single_target).float().item()\n",
    "\n",
    "    return batch_seq_loss, val_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Runs epoch w/ diff val \n",
    "'''\n",
    "def run_epoch(model, device, train_generator, val_generator, optimizer, num_val_batches=5, lr_scheduler=None, norm_list=[]):\n",
    "    model.to(device)\n",
    "    # model.train()\n",
    "    avg_total_loss = 0\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    val_metrics_dict = {\"quarter_output_pred\":0,\n",
    "                   \"halfway_output_pred\":0,\n",
    "                   \"three_quarter_output_pred\":0,\n",
    "                   \"final_output_pred\":0,\n",
    "                   \"correct_tackler_identified_w_highest_prob_anytime\":0,\n",
    "                   \"correct_tackler_had_highest_average_prob\":0,\n",
    "                   \"correct_tackler_average_prob\":0}\n",
    "    # train\n",
    "    for batch_index, (local_batch, local_labels, local_context, local_lengths, local_player_ids, local_ids) in enumerate(train_generator):\n",
    "        optimizer.zero_grad()\n",
    "        batch_train_loss = process_single_frame_no_seq_batch(model, device, local_batch, local_labels, local_context, local_player_ids, norm_list)\n",
    "        \n",
    "        train_loss_hist.append(batch_train_loss.item())\n",
    "        avg_total_loss += batch_train_loss.item()\n",
    "\n",
    "        batch_train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 250 == 0:\n",
    "            print(f\"batch {batch_index} train loss = {batch_train_loss}\")\n",
    "\n",
    "    avg_total_loss /= len(train_generator)\n",
    "    if lr_scheduler != None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    # check val\n",
    "    model.eval()\n",
    "    avg_val_loss = 0  # avg avg seq loss (e.g, on expected sequence, average loss)\n",
    "\n",
    "    gen = iter(val_generator)\n",
    "    for batch_index in range(num_val_batches):\n",
    "        with torch.no_grad():\n",
    "            local_batch, local_labels, local_context, local_lengths, local_player_ids, local_ids = next(gen)\n",
    "            # val_metrics[0] = val loss\n",
    "            batch_seq_loss, val_batch_metrics_dict = process_single_frame_no_seq_val_batch(model, device, local_batch, local_labels, local_context, local_lengths, local_player_ids, norm_list)\n",
    "            val_loss_hist.append(batch_seq_loss.item())\n",
    "\n",
    "            avg_val_loss += batch_seq_loss.item()\n",
    "            val_metrics_dict['quarter_output_pred'] += val_batch_metrics_dict['quarter_output_pred']\n",
    "            val_metrics_dict['halfway_output_pred'] += val_batch_metrics_dict['halfway_output_pred']\n",
    "            val_metrics_dict['three_quarter_output_pred'] += val_batch_metrics_dict['three_quarter_output_pred']\n",
    "            val_metrics_dict['final_output_pred'] += val_batch_metrics_dict['final_output_pred']\n",
    "            val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] += val_batch_metrics_dict['correct_tackler_identified_w_highest_prob_anytime']\n",
    "            val_metrics_dict['correct_tackler_had_highest_average_prob'] += val_batch_metrics_dict['correct_tackler_had_highest_average_prob']\n",
    "            val_metrics_dict['correct_tackler_average_prob'] += val_batch_metrics_dict['correct_tackler_average_prob']\n",
    "\n",
    "    avg_val_loss /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['quarter_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['halfway_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['three_quarter_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['final_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_had_highest_average_prob'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_average_prob'] /= (num_val_batches*val_generator.batch_size)\n",
    "\n",
    "    return avg_total_loss, avg_val_loss, train_loss_hist, val_loss_hist, val_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"saved_models/{SAVE_DIR}/train_arrs/trans_all_players_val_batch_loss_hist.pickle\", 'rb') as fp:\n",
    "#     val_batch_loss_hist = pickle.load(fp)\n",
    "# with open(f\"saved_models/{SAVE_DIR}/train_arrs/trans_all_players_train_batch_loss_hist.pickle\", 'rb') as fp:\n",
    "#     train_batch_loss_hist = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting training...\")\n",
    "train_batch_loss_hist=[]\n",
    "val_batch_loss_hist=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_start_time = time.time()\n",
    "\n",
    "for epoch_index in range(NUM_EPOCHS):\n",
    "    train_loss, val_loss, tr_hist, val_hist, val_metrics_dict = run_epoch(model, DEVICE, full_train_gen, seq_validation_generator, \\\n",
    "                                    optimizer, lr_scheduler=lr_scheduler, num_val_batches=14, norm_list=norm_list)\n",
    "\n",
    "    train_batch_loss_hist += tr_hist\n",
    "    val_batch_loss_hist += val_hist\n",
    "    print(f\"Epoch={epoch_index}: train_loss={train_loss}, val_loss={val_loss}. LR={lr_scheduler.get_last_lr()}\")\n",
    "    print(f\"val metrics dict = \")\n",
    "    print(f\"{list(val_metrics_dict.keys())}\")\n",
    "    print(f\"{np.array(list(val_metrics_dict.values())).round(3)}\")\n",
    "    print(f\"#######################\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch_index,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss\n",
    "    }, f\"./saved_models/{SAVE_DIR}/weights/checks/model_checkpoint_{epoch_index}.pt\")\n",
    "    \n",
    "    \n",
    "total_end_time = time.time()\n",
    "print(f\"Finished training {NUM_EPOCHS} epochs in {round((total_end_time - total_start_time)/60, 3)} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./saved_models/{SAVE_DIR}/weights/checks/trans_all_players.pt\")\n",
    "with open(f\"saved_models/{SAVE_DIR}/train_arrs/trans_all_players_train_batch_loss_hist.pickle\", 'wb') as fp:\n",
    "    pickle.dump(train_batch_loss_hist, fp)\n",
    "with open(f\"saved_models/{SAVE_DIR}/train_arrs/trans_all_players_val_batch_loss_hist.pickle\", 'wb') as fp:\n",
    "    pickle.dump(val_batch_loss_hist, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {'batch_size': 4,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "seq_test_set = SeqContextDataset(test_IDS, data_dir=DATA_DIR)\n",
    "seq_test_generator = torch.utils.data.DataLoader(seq_test_set, **val_params)\n",
    "all_test_features, all_test_labels, all_test_context_vectors, all_test_ids = seq_test_set.get_all_features_and_labels()\n",
    "full_test_set = SingleFrameContextDataset(all_test_features, all_test_labels, all_test_context_vectors, all_test_ids)\n",
    "full_test_gen = torch.utils.data.DataLoader(full_test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"saved_models/trans_single_frame_13_6_nine/test_IDS.npy\", test_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_test_epoch(model, device, test_generator, norm_list=[]):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    avg_val_loss = 0  # avg avg seq loss (e.g, on expected sequence, average loss)\n",
    "    val_loss_hist = []\n",
    "    gen = iter(test_generator)\n",
    "\n",
    "    num_batches = len(test_generator)\n",
    "    for batch_index in range(num_batches):\n",
    "        with torch.no_grad():\n",
    "            local_batch, local_labels, local_context, local_lengths, local_player_ids, local_ids = next(gen)\n",
    "            # val_metrics[0] = val loss\n",
    "            batch_seq_loss, val_batch_metrics_dict = process_single_frame_no_seq_val_batch(model, device, local_batch, local_labels, local_context, local_lengths, local_player_ids, norm_list)\n",
    "            val_loss_hist.append(batch_seq_loss.item())\n",
    "\n",
    "            avg_val_loss += batch_seq_loss.item()\n",
    "            val_metrics_dict['quarter_output_pred'] += val_batch_metrics_dict['quarter_output_pred']\n",
    "            val_metrics_dict['halfway_output_pred'] += val_batch_metrics_dict['halfway_output_pred']\n",
    "            val_metrics_dict['three_quarter_output_pred'] += val_batch_metrics_dict['three_quarter_output_pred']\n",
    "            val_metrics_dict['final_output_pred'] += val_batch_metrics_dict['final_output_pred']\n",
    "            val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] += val_batch_metrics_dict['correct_tackler_identified_w_highest_prob_anytime']\n",
    "            val_metrics_dict['correct_tackler_had_highest_average_prob'] += val_batch_metrics_dict['correct_tackler_had_highest_average_prob']\n",
    "            val_metrics_dict['correct_tackler_average_prob'] += val_batch_metrics_dict['correct_tackler_average_prob']\n",
    "\n",
    "    avg_val_loss /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['quarter_output_pred'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['halfway_output_pred'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['three_quarter_output_pred'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['final_output_pred'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_had_highest_average_prob'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_average_prob'] /= (num_batches*test_generator.batch_size)\n",
    "\n",
    "    return avg_val_loss, val_loss_hist, val_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loss, test_hist, test_metrics_dict = run_test_epoch(model, DEVICE, seq_test_generator, norm_list=norm_list)\n",
    "\n",
    "print(f\"Epoch={epoch_index}: test loss={test_loss}\")\n",
    "print(f\"test metrics dict = \")\n",
    "print(f\"{list(test_metrics_dict.keys())}\")\n",
    "print(f\"{np.array(list(test_metrics_dict.values())).round(3)}\")\n",
    "print(f\"#######################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdb_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
