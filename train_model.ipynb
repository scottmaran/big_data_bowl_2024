{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nUseful training tips: https://cs231n.github.io/neural-networks-3/\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Useful training tips: https://cs231n.github.io/neural-networks-3/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from models import trans_single_frame_13_6_nine, trans_single_frame_13_6_hw\n",
    "from utils import return_pos_embeddings, return_pos_df\n",
    "from dataset import SeqContextDataset, SingleFrameContextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13232c7d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = mps\n"
     ]
    }
   ],
   "source": [
    "MAX_FRAME_ID = 164\n",
    "TRAIN_SPLIT = 0.8\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "DATA_DIR = f\"seq_clipped_sorted_data\"\n",
    "\n",
    "NORM=True\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device = {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = f\"trans_single_frame_13_6_hw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings = return_pos_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = new_embeddings.loc[:,['0', '1']].max(axis=0).values\n",
    "min = new_embeddings.loc[:,['0', '1']].min(axis=0).values\n",
    "mean = new_embeddings.loc[:,['0', '1']].mean(axis=0).values\n",
    "new_embeddings.loc[:,['0','1']] = (new_embeddings.loc[:,['0','1']] - mean)/(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trans_single_frame_13_6_hw(pos_df=new_embeddings, feature_embed_size=128, dropout=0.35, num_encoder_layers=4, num_att_heads=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"./saved_models/trans_single_frame_12_12_pos/weights/trans_all_players.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=10, max_iters=2000)\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "lr_scheduler = ExponentialLR(optimizer, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'cleaned_data/{DATA_DIR}/game_play_id.json') as f:\n",
    "    list_IDS = json.load(f)\n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(list_IDS)\n",
    "\n",
    "# break ids into train-val-test sets\n",
    "val_percent = int(len(list_IDS)* (TRAIN_SPLIT+((1-TRAIN_SPLIT)/2)) )\n",
    "train_IDS, val_IDS, test_IDS = np.split(list_IDS, [ int(len(list_IDS)*TRAIN_SPLIT), val_percent ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 256,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "# Generators\n",
    "seq_training_set = SeqContextDataset(train_IDS, data_dir=DATA_DIR)\n",
    "seq_training_generator = torch.utils.data.DataLoader(seq_training_set, **params)\n",
    "\n",
    "val_params = {'batch_size': 4,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "seq_validation_set = SeqContextDataset(val_IDS, data_dir=DATA_DIR)\n",
    "seq_validation_generator = torch.utils.data.DataLoader(seq_validation_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2022100208_3722', '2022100202_2263', '2022103006_1856',\n",
       "       '2022101605_2298', '2022092510_55'], dtype='<U15')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_IDS[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_list = []\n",
    "if NORM==True:\n",
    "    training_mean, training_var, normalization_mask, min_max_diff = seq_training_set.get_normalize()\n",
    "    norm_list = [training_mean, training_var, normalization_mask, min_max_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_mean shape = torch.Size([253])\n",
      "normalization_mask shape = torch.Size([253])\n",
      "min_max_diff shape = torch.Size([1, 253])\n"
     ]
    }
   ],
   "source": [
    "print(f\"training_mean shape = {training_mean.shape}\")\n",
    "print(f\"normalization_mask shape = {normalization_mask.shape}\")\n",
    "print(f\"min_max_diff shape = {min_max_diff.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"saved_models/{SAVE_DIR}/weights/norm_list.pickle\", 'wb') as fp:\n",
    "   pickle.dump(norm_list, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_features, all_train_labels, all_train_context_vectors, all_train_ids = seq_training_set.get_all_features_and_labels()\n",
    "all_validation_features, all_validation_labels, all_val_context_vectors, all_val_ids = seq_validation_set.get_all_features_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new generator with single frames\n",
    "\n",
    "full_training_set = SingleFrameContextDataset(all_train_features, all_train_labels, all_train_context_vectors, all_train_ids)\n",
    "full_train_gen = torch.utils.data.DataLoader(full_training_set, **params)\n",
    "\n",
    "full_validation_set = SingleFrameContextDataset(all_validation_features, all_validation_labels, all_val_context_vectors, all_val_ids)\n",
    "full_validation_gen = torch.utils.data.DataLoader(full_validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_batch, local_labels, local_context, local_lengths, local_player_ids, local_ids = full_training_set.__getitem__(full_training_set.__len__()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000e+00,  3.8588e+04,  4.0074e+04,  4.3306e+04,  4.5063e+04,\n",
       "         4.6159e+04,  4.7788e+04,  4.7877e+04,  4.7882e+04,  4.7890e+04,\n",
       "         5.4498e+04,  5.4622e+04,  3.8557e+04,  4.1295e+04,  4.3295e+04,\n",
       "         4.3415e+04,  4.5268e+04,  4.6101e+04,  4.7852e+04,  4.7896e+04,\n",
       "         4.7906e+04,  5.4490e+04,  5.4604e+04], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_player_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len training set = 291367\n",
      "Len validation set = 37970\n",
      "Number of train batches = 1139\n",
      "Number of val batches = 149\n"
     ]
    }
   ],
   "source": [
    "print(f\"Len training set = {len(full_training_set)}\")\n",
    "print(f\"Len validation set = {len(full_validation_set)}\")\n",
    "\n",
    "print(f\"Number of train batches = {len(full_train_gen)}\")\n",
    "print(f\"Number of val batches = {len(full_validation_gen)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del seq_training_set\n",
    "#del training_generator\n",
    "del seq_validation_set\n",
    "#del validation_generator\n",
    "\n",
    "del all_train_features\n",
    "del all_train_labels\n",
    "del all_validation_features\n",
    "del all_validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Processes batch from generator for a single frame\n",
    "norm_list: if supplied, contains [training_mean, training_var, normalization_mask]\n",
    "'''\n",
    "def process_single_frame_no_seq_batch(model, device, batch, labels, context, player_ids, norm_list:list=[]):\n",
    "\n",
    "    local_batch = batch.to(device)\n",
    "    index_labels = torch.argmax(labels, -1).to(device) # (batch_size)\n",
    "    context = context.to(torch.float32).to(device)\n",
    "\n",
    "    # normalize, either mean/std or min_max\n",
    "    if len(norm_list) != 0:\n",
    "        training_mean, _, normalization_mask, min_max_diff = norm_list\n",
    "        training_mean = training_mean.to(device)\n",
    "        normalization_mask = normalization_mask.to(device)\n",
    "        min_max_diff = min_max_diff.to(device)\n",
    "        local_batch[:,normalization_mask] = (local_batch[:,normalization_mask] - training_mean[normalization_mask].reshape(1,-1))/torch.where(min_max_diff[:,normalization_mask]==0,1,min_max_diff[:,normalization_mask])\n",
    "\n",
    "    local_batch = local_batch.reshape(-1,23,11)\n",
    "\n",
    "    pos_embeddings = return_pos_embeddings(model.pos_df, player_ids).to(device)    # [batch_size, 23, embed_dim]\n",
    "\n",
    "    output = model(local_batch, pos_embeddings, context).to(device)  # (batch_size, 23)\n",
    "\n",
    "    batch_loss = F.nll_loss(output, index_labels, reduction='mean')\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_frame_no_seq_val_batch(model, device, batch, labels, context, lengths, player_ids, norm_list:list=[]):\n",
    "\n",
    "    local_batch = batch.to(device)\n",
    "    index_labels = torch.argmax(labels, -1).to(device) # (batch_size)\n",
    "    local_lengths = lengths.to(device)    # [batch_size, 23, embed_dim]\n",
    "    local_context = context.to(torch.float32).to(device)\n",
    "\n",
    "    batch_seq_loss = 0  # avg avg seq loss (e.g, on expected sequence, average loss)\n",
    "    val_metrics_dict = {\"quarter_output_pred\":0,\n",
    "                   \"halfway_output_pred\":0,\n",
    "                   \"three_quarter_output_pred\":0,\n",
    "                   \"final_output_pred\":0,\n",
    "                   \"correct_tackler_identified_w_highest_prob_anytime\":0,\n",
    "                   \"correct_tackler_had_highest_average_prob\":0,\n",
    "                   \"correct_tackler_average_prob\":0}\n",
    "\n",
    "    # for each full sequence\n",
    "    for i in range(0, local_batch.shape[0]):\n",
    "\n",
    "        single_target = index_labels[i, local_lengths[i]-1].to(device)                                 # ([]), the correct class\n",
    "        batch = local_batch[i, :local_lengths[i]-1, :].reshape(-1,23,11).to(device)    # (seq_length, 23, 11)\n",
    "        context = local_context[i,:local_lengths[i]-1,:].to(device)\n",
    "        pos_embeddings = return_pos_embeddings(model.pos_df, player_ids[i:i+1]).repeat(local_lengths[i]-1,1,1).to(device)\n",
    "\n",
    "        if len(norm_list) != 0:\n",
    "            training_mean, _, normalization_mask, min_max_diff = norm_list\n",
    "            training_mean = training_mean.to(device)\n",
    "            normalization_mask = normalization_mask.to(device)\n",
    "            min_max_diff = min_max_diff.to(device)\n",
    "\n",
    "            batch = batch.reshape(-1,23*11)\n",
    "            batch[:,normalization_mask] = (batch[:,normalization_mask] - training_mean[normalization_mask].reshape(1,-1))/torch.where(min_max_diff[:,normalization_mask]==0,1,min_max_diff[:,normalization_mask])\n",
    "            batch = batch.reshape(-1,23,11)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(batch, pos_embeddings, context).to(device)\n",
    "\n",
    "        batch_seq_loss += F.nll_loss(output, index_labels[i, :local_lengths[i]-1], reduction='mean')\n",
    "\n",
    "        val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] += ((output.argmax(dim=1) == single_target).sum() > 0).float().item()    #output.argmax(dim=1) = (seq_length)\n",
    "\n",
    "        avg_tackle_probs_over_seq = torch.exp(output.mean(dim=0))                                      # (24)\n",
    "        val_metrics_dict['correct_tackler_average_prob'] += avg_tackle_probs_over_seq[single_target].item()\n",
    "        val_metrics_dict['correct_tackler_had_highest_average_prob'] += (avg_tackle_probs_over_seq.argmax()==single_target).float().item()\n",
    "\n",
    "        # shape ([]), 1 or 0 if classified correctly at that point in time\n",
    "        val_metrics_dict['quarter_output_pred'] += (output[output.shape[0]//4, :].argmax() == single_target).float().item()\n",
    "        val_metrics_dict['halfway_output_pred'] += (output[output.shape[0]//2, :].argmax() == single_target).float().item()\n",
    "        val_metrics_dict['three_quarter_output_pred'] += (output[(output.shape[0]//4)*3, :].argmax() == single_target).float().item()\n",
    "        val_metrics_dict['final_output_pred'] += (output[-1, :].argmax() == single_target).float().item()\n",
    "\n",
    "    return batch_seq_loss, val_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Runs epoch w/ diff val \n",
    "'''\n",
    "def run_epoch(model, device, train_generator, val_generator, optimizer, num_val_batches=5, lr_scheduler=None, norm_list=[]):\n",
    "    model.to(device)\n",
    "    # model.train()\n",
    "    avg_total_loss = 0\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    val_metrics_dict = {\"quarter_output_pred\":0,\n",
    "                   \"halfway_output_pred\":0,\n",
    "                   \"three_quarter_output_pred\":0,\n",
    "                   \"final_output_pred\":0,\n",
    "                   \"correct_tackler_identified_w_highest_prob_anytime\":0,\n",
    "                   \"correct_tackler_had_highest_average_prob\":0,\n",
    "                   \"correct_tackler_average_prob\":0}\n",
    "    # train\n",
    "    for batch_index, (local_batch, local_labels, local_context, local_lengths, local_player_ids, local_ids) in enumerate(train_generator):\n",
    "        optimizer.zero_grad()\n",
    "        batch_train_loss = process_single_frame_no_seq_batch(model, device, local_batch, local_labels, local_context, local_player_ids, norm_list)\n",
    "        \n",
    "        train_loss_hist.append(batch_train_loss.item())\n",
    "        avg_total_loss += batch_train_loss.item()\n",
    "\n",
    "        batch_train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 250 == 0:\n",
    "            print(f\"batch {batch_index} train loss = {batch_train_loss}\")\n",
    "\n",
    "    avg_total_loss /= len(train_generator)\n",
    "    if lr_scheduler != None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    # check val\n",
    "    model.eval()\n",
    "    avg_val_loss = 0  # avg avg seq loss (e.g, on expected sequence, average loss)\n",
    "\n",
    "    gen = iter(val_generator)\n",
    "    for batch_index in range(num_val_batches):\n",
    "        with torch.no_grad():\n",
    "            local_batch, local_labels, local_context, local_lengths, local_player_ids, local_ids = next(gen)\n",
    "            # val_metrics[0] = val loss\n",
    "            batch_seq_loss, val_batch_metrics_dict = process_single_frame_no_seq_val_batch(model, device, local_batch, local_labels, local_context, local_lengths, local_player_ids, norm_list)\n",
    "            val_loss_hist.append(batch_seq_loss.item())\n",
    "\n",
    "            avg_val_loss += batch_seq_loss.item()\n",
    "            val_metrics_dict['quarter_output_pred'] += val_batch_metrics_dict['quarter_output_pred']\n",
    "            val_metrics_dict['halfway_output_pred'] += val_batch_metrics_dict['halfway_output_pred']\n",
    "            val_metrics_dict['three_quarter_output_pred'] += val_batch_metrics_dict['three_quarter_output_pred']\n",
    "            val_metrics_dict['final_output_pred'] += val_batch_metrics_dict['final_output_pred']\n",
    "            val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] += val_batch_metrics_dict['correct_tackler_identified_w_highest_prob_anytime']\n",
    "            val_metrics_dict['correct_tackler_had_highest_average_prob'] += val_batch_metrics_dict['correct_tackler_had_highest_average_prob']\n",
    "            val_metrics_dict['correct_tackler_average_prob'] += val_batch_metrics_dict['correct_tackler_average_prob']\n",
    "\n",
    "    avg_val_loss /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['quarter_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['halfway_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['three_quarter_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['final_output_pred'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_had_highest_average_prob'] /= (num_val_batches*val_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_average_prob'] /= (num_val_batches*val_generator.batch_size)\n",
    "\n",
    "    return avg_total_loss, avg_val_loss, train_loss_hist, val_loss_hist, val_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"saved_models/{SAVE_DIR}/train_arrs/trans_all_players_val_batch_loss_hist.pickle\", 'rb') as fp:\n",
    "#     val_batch_loss_hist = pickle.load(fp)\n",
    "# with open(f\"saved_models/{SAVE_DIR}/train_arrs/trans_all_players_train_batch_loss_hist.pickle\", 'rb') as fp:\n",
    "#     train_batch_loss_hist = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting training...\")\n",
    "train_batch_loss_hist=[]\n",
    "val_batch_loss_hist=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/4wq7jf9n3c74v00gxl_2v81r0000gn/T/ipykernel_36985/2075596127.py:17: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:283.)\n",
      "  local_batch[:,normalization_mask] = (local_batch[:,normalization_mask] - training_mean[normalization_mask].reshape(1,-1))/torch.where(min_max_diff[:,normalization_mask]==0,1,min_max_diff[:,normalization_mask])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 train loss = 3.133817195892334\n",
      "batch 250 train loss = 2.0665488243103027\n",
      "batch 500 train loss = 1.8899847269058228\n",
      "batch 750 train loss = 1.7359659671783447\n",
      "batch 1000 train loss = 1.7755177021026611\n",
      "Epoch=0: train_loss=1.9103833276834061, val_loss=1.585815898009709. LR=[0.00015]\n",
      "val metrics dict = \n",
      "['quarter_output_pred', 'halfway_output_pred', 'three_quarter_output_pred', 'final_output_pred', 'correct_tackler_identified_w_highest_prob_anytime', 'correct_tackler_had_highest_average_prob', 'correct_tackler_average_prob']\n",
      "[0.321 0.393 0.554 0.518 0.839 0.482 0.31 ]\n",
      "#######################\n",
      "batch 0 train loss = 1.6663129329681396\n",
      "batch 250 train loss = 1.4966011047363281\n",
      "batch 500 train loss = 1.4492875337600708\n",
      "batch 750 train loss = 1.4509553909301758\n",
      "batch 1000 train loss = 1.4340709447860718\n",
      "Epoch=1: train_loss=1.4662454734254657, val_loss=1.346312118428094. LR=[7.5e-05]\n",
      "val metrics dict = \n",
      "['quarter_output_pred', 'halfway_output_pred', 'three_quarter_output_pred', 'final_output_pred', 'correct_tackler_identified_w_highest_prob_anytime', 'correct_tackler_had_highest_average_prob', 'correct_tackler_average_prob']\n",
      "[0.411 0.554 0.768 0.679 0.911 0.679 0.35 ]\n",
      "#######################\n",
      "batch 0 train loss = 1.485487937927246\n",
      "batch 250 train loss = 1.3196618556976318\n",
      "batch 500 train loss = 1.3313966989517212\n",
      "batch 750 train loss = 1.2022709846496582\n",
      "batch 1000 train loss = 1.30584716796875\n",
      "Epoch=2: train_loss=1.2646546258122593, val_loss=1.3629394939967565. LR=[3.75e-05]\n",
      "val metrics dict = \n",
      "['quarter_output_pred', 'halfway_output_pred', 'three_quarter_output_pred', 'final_output_pred', 'correct_tackler_identified_w_highest_prob_anytime', 'correct_tackler_had_highest_average_prob', 'correct_tackler_average_prob']\n",
      "[0.357 0.464 0.679 0.571 0.946 0.786 0.348]\n",
      "#######################\n",
      "batch 0 train loss = 1.2651171684265137\n",
      "batch 250 train loss = 1.2413700819015503\n",
      "batch 500 train loss = 1.192387342453003\n",
      "batch 750 train loss = 1.1258426904678345\n",
      "batch 1000 train loss = 1.020720362663269\n",
      "Epoch=3: train_loss=1.0907347840212018, val_loss=1.5433971754142217. LR=[1.875e-05]\n",
      "val metrics dict = \n",
      "['quarter_output_pred', 'halfway_output_pred', 'three_quarter_output_pred', 'final_output_pred', 'correct_tackler_identified_w_highest_prob_anytime', 'correct_tackler_had_highest_average_prob', 'correct_tackler_average_prob']\n",
      "[0.339 0.536 0.589 0.589 0.946 0.768 0.343]\n",
      "#######################\n",
      "batch 0 train loss = 0.9132518172264099\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb Cell 31\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m total_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_loss, val_loss, tr_hist, val_hist, val_metrics_dict \u001b[39m=\u001b[39m run_epoch(model, DEVICE, full_train_gen, seq_validation_generator, \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                     optimizer, lr_scheduler\u001b[39m=\u001b[39;49mlr_scheduler, num_val_batches\u001b[39m=\u001b[39;49m\u001b[39m14\u001b[39;49m, norm_list\u001b[39m=\u001b[39;49mnorm_list)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_batch_loss_hist \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_hist\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     val_batch_loss_hist \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m val_hist\n",
      "\u001b[1;32m/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m batch_train_loss \u001b[39m=\u001b[39m process_single_frame_no_seq_batch(model, device, local_batch, local_labels, local_context, local_player_ids, norm_list)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m train_loss_hist\u001b[39m.\u001b[39mappend(batch_train_loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m avg_total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_train_loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/scottmaran/Code/Python/bdb_2024/train_pos_embed_single_frame.ipynb#X50sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m batch_train_loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_start_time = time.time()\n",
    "\n",
    "for epoch_index in range(NUM_EPOCHS):\n",
    "    train_loss, val_loss, tr_hist, val_hist, val_metrics_dict = run_epoch(model, DEVICE, full_train_gen, seq_validation_generator, \\\n",
    "                                    optimizer, lr_scheduler=lr_scheduler, num_val_batches=14, norm_list=norm_list)\n",
    "\n",
    "    train_batch_loss_hist += tr_hist\n",
    "    val_batch_loss_hist += val_hist\n",
    "    print(f\"Epoch={epoch_index}: train_loss={train_loss}, val_loss={val_loss}. LR={lr_scheduler.get_last_lr()}\")\n",
    "    print(f\"val metrics dict = \")\n",
    "    print(f\"{list(val_metrics_dict.keys())}\")\n",
    "    print(f\"{np.array(list(val_metrics_dict.values())).round(3)}\")\n",
    "    print(f\"#######################\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch_index,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss\n",
    "    }, f\"./saved_models/{SAVE_DIR}/weights/checks/model_checkpoint_{epoch_index}.pt\")\n",
    "    \n",
    "    \n",
    "total_end_time = time.time()\n",
    "print(f\"Finished training {NUM_EPOCHS} epochs in {round((total_end_time - total_start_time)/60, 3)} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./saved_models/{SAVE_DIR}/weights/checks/trans_all_players.pt\")\n",
    "with open(f\"saved_models/{SAVE_DIR}/train_arrs/trans_all_players_train_batch_loss_hist.pickle\", 'wb') as fp:\n",
    "    pickle.dump(train_batch_loss_hist, fp)\n",
    "with open(f\"saved_models/{SAVE_DIR}/train_arrs/trans_all_players_val_batch_loss_hist.pickle\", 'wb') as fp:\n",
    "    pickle.dump(val_batch_loss_hist, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {'batch_size': 4,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "seq_test_set = SeqContextDataset(test_IDS, data_dir=DATA_DIR)\n",
    "seq_test_generator = torch.utils.data.DataLoader(seq_test_set, **val_params)\n",
    "all_test_features, all_test_labels, all_test_context_vectors, all_test_ids = seq_test_set.get_all_features_and_labels()\n",
    "full_test_set = SingleFrameContextDataset(all_test_features, all_test_labels, all_test_context_vectors, all_test_ids)\n",
    "full_test_gen = torch.utils.data.DataLoader(full_test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"saved_models/trans_single_frame_13_6_nine/test_IDS.npy\", test_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_test_epoch(model, device, test_generator, norm_list=[]):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    avg_val_loss = 0  # avg avg seq loss (e.g, on expected sequence, average loss)\n",
    "    val_loss_hist = []\n",
    "    gen = iter(test_generator)\n",
    "\n",
    "    num_batches = len(test_generator)\n",
    "    for batch_index in range(num_batches):\n",
    "        with torch.no_grad():\n",
    "            local_batch, local_labels, local_context, local_lengths, local_player_ids, local_ids = next(gen)\n",
    "            # val_metrics[0] = val loss\n",
    "            batch_seq_loss, val_batch_metrics_dict = process_single_frame_no_seq_val_batch(model, device, local_batch, local_labels, local_context, local_lengths, local_player_ids, norm_list)\n",
    "            val_loss_hist.append(batch_seq_loss.item())\n",
    "\n",
    "            avg_val_loss += batch_seq_loss.item()\n",
    "            val_metrics_dict['quarter_output_pred'] += val_batch_metrics_dict['quarter_output_pred']\n",
    "            val_metrics_dict['halfway_output_pred'] += val_batch_metrics_dict['halfway_output_pred']\n",
    "            val_metrics_dict['three_quarter_output_pred'] += val_batch_metrics_dict['three_quarter_output_pred']\n",
    "            val_metrics_dict['final_output_pred'] += val_batch_metrics_dict['final_output_pred']\n",
    "            val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] += val_batch_metrics_dict['correct_tackler_identified_w_highest_prob_anytime']\n",
    "            val_metrics_dict['correct_tackler_had_highest_average_prob'] += val_batch_metrics_dict['correct_tackler_had_highest_average_prob']\n",
    "            val_metrics_dict['correct_tackler_average_prob'] += val_batch_metrics_dict['correct_tackler_average_prob']\n",
    "\n",
    "    avg_val_loss /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['quarter_output_pred'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['halfway_output_pred'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['three_quarter_output_pred'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['final_output_pred'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_identified_w_highest_prob_anytime'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_had_highest_average_prob'] /= (num_batches*test_generator.batch_size)\n",
    "    val_metrics_dict['correct_tackler_average_prob'] /= (num_batches*test_generator.batch_size)\n",
    "\n",
    "    return avg_val_loss, val_loss_hist, val_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=2: test loss=1.2699910836765564\n",
      "test metrics dict = \n",
      "['quarter_output_pred', 'halfway_output_pred', 'three_quarter_output_pred', 'final_output_pred', 'correct_tackler_identified_w_highest_prob_anytime', 'correct_tackler_had_highest_average_prob', 'correct_tackler_average_prob']\n",
      "[0.412 0.544 0.696 0.71  0.949 0.729 0.386]\n",
      "#######################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_hist, test_metrics_dict = run_test_epoch(model, DEVICE, seq_test_generator, norm_list=norm_list)\n",
    "\n",
    "print(f\"Epoch={epoch_index}: test loss={test_loss}\")\n",
    "print(f\"test metrics dict = \")\n",
    "print(f\"{list(test_metrics_dict.keys())}\")\n",
    "print(f\"{np.array(list(test_metrics_dict.values())).round(3)}\")\n",
    "print(f\"#######################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdb_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
